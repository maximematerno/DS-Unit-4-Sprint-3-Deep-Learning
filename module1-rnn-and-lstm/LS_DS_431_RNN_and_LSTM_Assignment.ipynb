{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 3 Assignment 1*\n",
    "\n",
    "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
    "\n",
    "![Monkey at a typewriter](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/603px-Chimpanzee_seated_at_typewriter.jpg)\n",
    "\n",
    "It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
    "\n",
    "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
    "\n",
    "Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n",
    "\n",
    "Then, use that trained RNN to generate Shakespearean-ish text. Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n",
    "\n",
    "Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running. Then, once you've got a proof of concept - start pushing it more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ltj1je1fp5rO"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.utils import to_categorical, get_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.gutenberg.org/files/100/100-0.txt\n",
      "5783552/5777367 [==============================] - 3s 0us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5740053"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://www.gutenberg.org/files/100/100-0.txt'\n",
    "\n",
    "doc = get_file('shakespeare.txt', url)\n",
    "text = open(doc, 'rb').read().decode(encoding='utf-8-sig')\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the text \n",
    "\n",
    "# Cleaning\n",
    "text = text.replace('\\r', '')\n",
    "''' Lower casing Text '''\n",
    "text = text[900:-25000].lower()\n",
    "''' Fixing the spacing Issue '''\n",
    "text = ' '.join(text.split())\n",
    "\n",
    "\n",
    "# Size\n",
    "text_size = len(text)\n",
    "# All the letters/characters used in the Text\n",
    "vocab = sorted(set(text))\n",
    "\n",
    "# All the letters/characters into ints \n",
    "char_to_int = {c:i for i, c in enumerate(vocab)}\n",
    "int_to_char = {i:c for i, c in enumerate(vocab)}\n",
    "\n",
    "text_integers = np.array([char_to_int[c] for c in text])\n",
    "\n",
    "# Epoch \n",
    "seq_length = 100\n",
    "\n",
    "X_text = []\n",
    "y_text = []\n",
    "\n",
    "for i in range(0, 100000 - seq_length,1):\n",
    "    in_seq = text[i:i + seq_length]\n",
    "    out_char = text[i + seq_length]\n",
    "    X_text.append([char_to_int[char] for char in in_seq])\n",
    "    y_text.append(char_to_int[out_char])\n",
    "    \n",
    "samples = len(X_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99900"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99900, 71)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.reshape(X_text, (99900, 100, 1))\n",
    "X = X / len(vocab)\n",
    "\n",
    "y = to_categorical(y_text)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 100, 256)          264192    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100, 256)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 71)                18247     \n",
      "=================================================================\n",
      "Total params: 807,751\n",
      "Trainable params: 807,751\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape = (X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 99900 samples\n",
      "Epoch 1/50\n",
      "99900/99900 [==============================] - 540s 5ms/sample - loss: 3.0580\n",
      "Epoch 2/50\n",
      "99900/99900 [==============================] - 513s 5ms/sample - loss: 2.9629\n",
      "Epoch 3/50\n",
      "99900/99900 [==============================] - 508s 5ms/sample - loss: 2.9007\n",
      "Epoch 4/50\n",
      "99900/99900 [==============================] - 1320s 13ms/sample - loss: 2.7901\n",
      "Epoch 5/50\n",
      "99900/99900 [==============================] - 515s 5ms/sample - loss: 2.7311\n",
      "Epoch 6/50\n",
      "99900/99900 [==============================] - 1021s 10ms/sample - loss: 2.6911\n",
      "Epoch 7/50\n",
      "99900/99900 [==============================] - 516s 5ms/sample - loss: 2.6611\n",
      "Epoch 8/50\n",
      "99900/99900 [==============================] - 515s 5ms/sample - loss: 2.6315\n",
      "Epoch 9/50\n",
      "99900/99900 [==============================] - 514s 5ms/sample - loss: 2.6017\n",
      "Epoch 10/50\n",
      "99900/99900 [==============================] - 541s 5ms/sample - loss: 2.5741\n",
      "Epoch 11/50\n",
      "99900/99900 [==============================] - 514s 5ms/sample - loss: 2.5452\n",
      "Epoch 12/50\n",
      "99900/99900 [==============================] - 512s 5ms/sample - loss: 2.5195\n",
      "Epoch 13/50\n",
      "99900/99900 [==============================] - 511s 5ms/sample - loss: 2.4984\n",
      "Epoch 14/50\n",
      "99900/99900 [==============================] - 510s 5ms/sample - loss: 2.4801\n",
      "Epoch 15/50\n",
      "99900/99900 [==============================] - 542s 5ms/sample - loss: 2.4593\n",
      "Epoch 16/50\n",
      "99900/99900 [==============================] - 544s 5ms/sample - loss: 2.4381\n",
      "Epoch 17/50\n",
      "99900/99900 [==============================] - 515s 5ms/sample - loss: 2.4186\n",
      "Epoch 18/50\n",
      "99900/99900 [==============================] - 647s 6ms/sample - loss: 2.4014\n",
      "Epoch 19/50\n",
      "99900/99900 [==============================] - 755s 8ms/sample - loss: 2.3805\n",
      "Epoch 20/50\n",
      "99900/99900 [==============================] - 528s 5ms/sample - loss: 2.3618\n",
      "Epoch 21/50\n",
      "99900/99900 [==============================] - 571s 6ms/sample - loss: 2.3443\n",
      "Epoch 22/50\n",
      "99900/99900 [==============================] - 623s 6ms/sample - loss: 2.3212\n",
      "Epoch 23/50\n",
      "99900/99900 [==============================] - 518s 5ms/sample - loss: 2.3072\n",
      "Epoch 24/50\n",
      "99900/99900 [==============================] - 515s 5ms/sample - loss: 2.2854\n",
      "Epoch 25/50\n",
      "99900/99900 [==============================] - 518s 5ms/sample - loss: 2.2676\n",
      "Epoch 26/50\n",
      "99900/99900 [==============================] - 518s 5ms/sample - loss: 2.2512\n",
      "Epoch 27/50\n",
      "99900/99900 [==============================] - 521s 5ms/sample - loss: 2.2334\n",
      "Epoch 28/50\n",
      "99900/99900 [==============================] - 522s 5ms/sample - loss: 2.2215\n",
      "Epoch 29/50\n",
      "99900/99900 [==============================] - 523s 5ms/sample - loss: 2.2049\n",
      "Epoch 30/50\n",
      "99900/99900 [==============================] - 523s 5ms/sample - loss: 2.1871\n",
      "Epoch 31/50\n",
      "99900/99900 [==============================] - 523s 5ms/sample - loss: 2.1693\n",
      "Epoch 32/50\n",
      "99900/99900 [==============================] - 523s 5ms/sample - loss: 2.1569\n",
      "Epoch 33/50\n",
      "99900/99900 [==============================] - 523s 5ms/sample - loss: 2.1406\n",
      "Epoch 34/50\n",
      "99900/99900 [==============================] - 524s 5ms/sample - loss: 2.1254\n",
      "Epoch 35/50\n",
      "99900/99900 [==============================] - 525s 5ms/sample - loss: 2.1103\n",
      "Epoch 36/50\n",
      "99900/99900 [==============================] - 525s 5ms/sample - loss: 2.0968\n",
      "Epoch 37/50\n",
      "99900/99900 [==============================] - 524s 5ms/sample - loss: 2.0795\n",
      "Epoch 38/50\n",
      "99900/99900 [==============================] - 515s 5ms/sample - loss: 2.0659\n",
      "Epoch 39/50\n",
      "99900/99900 [==============================] - 3556s 36ms/sample - loss: 2.0582\n",
      "Epoch 40/50\n",
      "99900/99900 [==============================] - 3260s 33ms/sample - loss: 2.0447\n",
      "Epoch 41/50\n",
      "99900/99900 [==============================] - 523s 5ms/sample - loss: 2.0247\n",
      "Epoch 42/50\n",
      "99900/99900 [==============================] - 522s 5ms/sample - loss: 2.0084\n",
      "Epoch 43/50\n",
      "99900/99900 [==============================] - 527s 5ms/sample - loss: 2.0000\n",
      "Epoch 44/50\n",
      "99900/99900 [==============================] - 526s 5ms/sample - loss: 1.9833\n",
      "Epoch 45/50\n",
      "99900/99900 [==============================] - 527s 5ms/sample - loss: 1.9753\n",
      "Epoch 46/50\n",
      "99900/99900 [==============================] - 536s 5ms/sample - loss: 1.9552\n",
      "Epoch 47/50\n",
      "99900/99900 [==============================] - 530s 5ms/sample - loss: 1.9423\n",
      "Epoch 48/50\n",
      "99900/99900 [==============================] - 528s 5ms/sample - loss: 1.9354\n",
      "Epoch 49/50\n",
      "99900/99900 [==============================] - 526s 5ms/sample - loss: 1.9182\n",
      "Epoch 50/50\n",
      "99900/99900 [==============================] - 527s 5ms/sample - loss: 1.8996\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metric=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "history = model.fit(X, y, batch_size=1000, epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: \n",
      " ly steep in a cold valley-fountain of that ground: which borrowed from this holy fire of love, a dat\n"
     ]
    }
   ],
   "source": [
    "# Generate Text\n",
    "\n",
    "import textwrap\n",
    "\n",
    "start = np.random.randint(0, len(X_text)-1)\n",
    "vocab_len = len(vocab)\n",
    "pattern = X_text[start]\n",
    "\n",
    "print(f\"Seed: \\n {''.join([int_to_char[value] for value in pattern])}\")\n",
    "out = [int_to_char[value] for value in pattern]\n",
    "\n",
    "# Generate characters\n",
    "for i in range(500):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(vocab_len)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    in_seq = [int_to_char[value] for value in pattern]\n",
    "    out.append(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "\n",
    "print('\\n')\n",
    "print(\"LSTM Generated Text:\\n\")\n",
    "print(textwrap.fill(''.join(out), 80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zE4a4O7Bp5x1"
   },
   "source": [
    "# Resources and Stretch Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uT3UV3gap9H6"
   },
   "source": [
    "## Stretch goals:\n",
    "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n",
    "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
    "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
    "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
    "- Run on bigger, better data\n",
    "\n",
    "## Resources:\n",
    "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - a seminal writeup demonstrating a simple but effective character-level NLP RNN\n",
    "- [Simple NumPy implementation of RNN](https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb) - Python 3 version of the code from \"Unreasonable Effectiveness\"\n",
    "- [TensorFlow RNN Tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn) - code for training a RNN on the Penn Tree Bank language dataset\n",
    "- [4 part tutorial on RNN](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) - relates RNN to the vanishing gradient problem, and provides example implementation\n",
    "- [RNN training tips and tricks](https://github.com/karpathy/char-rnn#tips-and-tricks) - some rules of thumb for parameterizing and training your RNN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
